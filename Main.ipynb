{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbb2f6a-4aa1-408c-a600-82e3695ccfdc",
   "metadata": {},
   "source": [
    "### Vamos a cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345e6a2a-1316-4d7b-9847-f7e884dc2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arreglo_datos import vectorDatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eeef525-dfc7-43e1-96d1-bccd217c351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\masan\\\\Desktop\\\\Mario\\\\ESFM\\\\Octavo Semestre\\\\Servicio Social\\\\Etiquetas'\n",
    "dic_nombre_marcas = vectorDatos(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d996ce-1aa4-45d7-93a6-34f4449fdf6f",
   "metadata": {},
   "source": [
    "#### Vamos a dividir los datos en conjunto de entrenamiento y testeo (80-20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea67635-c6bb-4630-b538-2a2424058c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a extraer los datos de cada imagen para poder hacer el split de datos.\n",
    "vectorDeDatos = []\n",
    "nombreDeImagenes = []\n",
    "for key in dic_nombre_marcas.keys():\n",
    "    vectorDeDatos.append(dic_nombre_marcas[key])\n",
    "    nombreDeImagenes.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58644e86-d8b1-45b7-86b7-51823096d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento = int(0.8*len(vectorDeDatos))\n",
    "datos_entrenamiento = vectorDeDatos[:entrenamiento]\n",
    "datos_testeo = vectorDeDatos[entrenamiento:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c96b539-6bbd-493b-9e3c-eafac918f7a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"legend.title_fontsize\" on line 22 in\n",
      "C:\\Users\\masan\\.matplotlib\\stylelib\\notebook.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transformacion(data, output_size=150):\n",
    "    \n",
    "    path = 'C:\\\\Users\\\\masan\\\\Desktop\\\\Mario\\\\ESFM\\\\Octavo Semestre\\\\Servicio Social\\\\imagenes'\n",
    "    nombre_imagen = data[-1][0]\n",
    "    puntos = np.array([])\n",
    "    \n",
    "    for fila_datos in data[:-1]:\n",
    "        puntos = np.append(puntos, np.array(fila_datos[5:]))\n",
    "    puntos = puntos.reshape(14,2)\n",
    "        \n",
    "    h, w = 540, 960\n",
    "    \n",
    "    if h > w:\n",
    "        new_h, new_w = output_size * h / w, output_size\n",
    "    else:\n",
    "        new_h, new_w = output_size, output_size * w / h\n",
    "        \n",
    "    new_h, new_w = int(new_h), int(new_w)\n",
    "    \n",
    "    img = Image.open(f'{path}\\\\{nombre_imagen}')\n",
    "    img = np.asarray(img)\n",
    "    img = cv2.resize(img, (new_w, new_h))\n",
    "    \n",
    "    # Pasamos a blanco y negro\n",
    "    image_copy = img.copy()    \n",
    "    image_copy = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "    image_copy=  image_copy/255.0    \n",
    "    img = image_copy\n",
    "    \n",
    "    # escalado de puntos\n",
    "    puntos = puntos * [new_w/w, new_h/h]\n",
    "    \n",
    "    # Pasamos la imagen y los puntos a tensor\n",
    "    if(len(img.shape) == 2):\n",
    "        img = img.reshape(img.shape[0], img.shape[1], 1)\n",
    "        \n",
    "    img = img.transpose((2, 0, 1))\n",
    "        \n",
    "    return {'image': torch.from_numpy(img), 'puntos': torch.from_numpy(puntos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8565d4bf-0f96-47c8-8e3f-2ae2769c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_transformados = []\n",
    "for i in range(len(datos_entrenamiento)):\n",
    "    sample = transformacion(datos_entrenamiento[i])\n",
    "    datos_transformados.append(sample)\n",
    "    #print(i, sample['image'].shape, sample['puntos'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78699cc-2528-410c-bae9-fcbb37279941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datos_transformados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f29da-50f6-42dc-b157-c28384ce729b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python cHido (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
